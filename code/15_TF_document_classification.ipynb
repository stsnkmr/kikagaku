{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_rPzMtR94ecT"
   },
   "source": [
    "# 文書分類\n",
    "\n",
    "前回までの自然言語処理の基礎を活かして、実際の文書分類を行うためのデータの準備を行いましょう。\n",
    "\n",
    "## 問題設定\n",
    "\n",
    "今回は、ニュース記事をカテゴリ分けする問題設定で考えていきます。\n",
    "まずはじめに、特定フォルダ内の記事を読み込む練習をしましょう。\n",
    "\n",
    "![](images/4-5/img.png)\n",
    "\n",
    "今回はこのように９つの記事カテゴリーを文章から分類していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-MDsZDcD4ecU"
   },
   "source": [
    "## データの前準備\n",
    "\n",
    "まず、zipファイルを解凍する必要があります。\n",
    "Jupyter Notebookでは、zipファイルなどで圧縮してからUploadする必要があるため、圧縮してUploadして解凍するといった操作を多用しますので、覚えておきましょう。\n",
    "\n",
    "`!` を使用することで、LinuxのBashコマンドを使用することができ、こちらで`unzip`コマンドを使うことで簡単にファイル操作が可能です。一番楽です。\n",
    "\n",
    "まずはファイルが格納されているか確認しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2595,
     "status": "ok",
     "timestamp": 1569055677279,
     "user": {
      "displayName": "佐川史弥",
      "photoUrl": "",
      "userId": "17735252083618003950"
     },
     "user_tz": -540
    },
    "id": "s6Cz3bZ04ecV",
    "outputId": "34b42628-19a7-4b8f-8b13-f7ac27df0de8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSubclassing.ipynb\u001b[m\u001b[m                   \u001b[31m_DS_Store\u001b[m\u001b[m\r\n",
      "SubclassingAPI.ipynb                \u001b[31mazure.ipynb\u001b[m\u001b[m\r\n",
      "\u001b[31mTF_Classification.ipynb\u001b[m\u001b[m             \u001b[34mdata\u001b[m\u001b[m\r\n",
      "\u001b[31mTF_Regression.ipynb\u001b[m\u001b[m                 \u001b[31mimage.ipynb\u001b[m\u001b[m\r\n",
      "\u001b[31mTF_Seq2Seq.ipynb\u001b[m\u001b[m                    \u001b[34mimages\u001b[m\u001b[m\r\n",
      "\u001b[31mTF_TensorBoard.ipynb\u001b[m\u001b[m                \u001b[31mintro.ipynb\u001b[m\u001b[m\r\n",
      "\u001b[31mTF_cat_dog.ipynb\u001b[m\u001b[m                    livedoordic.txt\r\n",
      "\u001b[31mTF_cnn_cifar10.ipynb\u001b[m\u001b[m                \u001b[34mpeachy\u001b[m\u001b[m\r\n",
      "\u001b[31mTF_cnn_mnist.ipynb\u001b[m\u001b[m                  \u001b[31mprogramming.ipynb\u001b[m\u001b[m\r\n",
      "\u001b[31mTF_document_classification.ipynb\u001b[m\u001b[m    \u001b[31mregistration.ipynb\u001b[m\u001b[m\r\n",
      "\u001b[31mTF_nlp.ipynb\u001b[m\u001b[m                        \u001b[31msend_img.py\u001b[m\u001b[m\r\n",
      "\u001b[31mTF_time_series.ipynb\u001b[m\u001b[m                \u001b[34mtext\u001b[m\u001b[m\r\n",
      "\u001b[31mTF_time_series_classification.ipynb\u001b[m\u001b[m \u001b[34mtopic-news\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip text.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9291,
     "status": "ok",
     "timestamp": 1569055683997,
     "user": {
      "displayName": "佐川史弥",
      "photoUrl": "",
      "userId": "17735252083618003950"
     },
     "user_tz": -540
    },
    "id": "7anI_rtJ4eca",
    "outputId": "788cf98c-0e61-43ca-913e-a99fefebba55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m.\u001b[m\u001b[m              \u001b[34mit-life-hack\u001b[m\u001b[m   \u001b[34mlivedoor-homme\u001b[m\u001b[m \u001b[34mpeachy\u001b[m\u001b[m         \u001b[34msports-watch\u001b[m\u001b[m\r\n",
      "\u001b[34m..\u001b[m\u001b[m             \u001b[34mkaden-channel\u001b[m\u001b[m  \u001b[34mmovie-enter\u001b[m\u001b[m    \u001b[34msmax\u001b[m\u001b[m           \u001b[34mtopic-news\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "# ファイルを確認\n",
    "!ls -a text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qa2AgEnP4ecc"
   },
   "source": [
    "`.DS_Store`という不必要なファイルが存在するため、消しておきましょう。\n",
    "圧縮のタイミングやOSによるものでいらないファイルが見つかるかもしれないため、フォルダの中などのデータをよく見ておきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k-ZJy-k-4ecc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: text/.DS_Store: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm text/.DS_Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12249,
     "status": "ok",
     "timestamp": 1569055686965,
     "user": {
      "displayName": "佐川史弥",
      "photoUrl": "",
      "userId": "17735252083618003950"
     },
     "user_tz": -540
    },
    "id": "2UjdyVNP4ece",
    "outputId": "338e6790-0b7b-4cba-d824-9a8167fac866"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m.\u001b[m\u001b[m              \u001b[34mit-life-hack\u001b[m\u001b[m   \u001b[34mlivedoor-homme\u001b[m\u001b[m \u001b[34mpeachy\u001b[m\u001b[m         \u001b[34msports-watch\u001b[m\u001b[m\r\n",
      "\u001b[34m..\u001b[m\u001b[m             \u001b[34mkaden-channel\u001b[m\u001b[m  \u001b[34mmovie-enter\u001b[m\u001b[m    \u001b[34msmax\u001b[m\u001b[m           \u001b[34mtopic-news\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "# ファイルを確認\n",
    "!ls -a text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oeW9Opba4ecf"
   },
   "source": [
    "このように削除できました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AaWPDdvq4ecg"
   },
   "source": [
    "## 各ディレクトリのテキスト情報を変数に読み込む\n",
    "\n",
    "このテキスト情報を読み込むときに、便利なものが、辞書型の変数になります。\n",
    "\n",
    "辞書型（data）のキーにファイル名、値にテキストの文字列を読み込みます。\n",
    "\n",
    "ファイルやディレクトリを操作するときにとても便利な`glob`というライブラリもインポートしておきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dm291Apw4ech"
   },
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-SB4ZGdn4eci"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12237,
     "status": "ok",
     "timestamp": 1569055686967,
     "user": {
      "displayName": "佐川史弥",
      "photoUrl": "",
      "userId": "17735252083618003950"
     },
     "user_tz": -540
    },
    "id": "Nm9XkmyR4eck",
    "outputId": "743da58d-db6c-4c2f-910e-517d1defa3b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text/smax',\n",
       " 'text/kaden-channel',\n",
       " 'text/livedoor-homme',\n",
       " 'text/movie-enter',\n",
       " 'text/sports-watch',\n",
       " 'text/it-life-hack',\n",
       " 'text/topic-news',\n",
       " 'text/peachy']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directories = glob('text/*')\n",
    "directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YibIsP9_4ecm"
   },
   "source": [
    "for文で繰り返すときに、各フォルダ（ディレクトリ）に対応する要素番号を分類に使用するラベルと設定したいため、`enumerate`を使うことで、要素番号も併せて取得しておきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12233,
     "status": "ok",
     "timestamp": 1569055686967,
     "user": {
      "displayName": "佐川史弥",
      "photoUrl": "",
      "userId": "17735252083618003950"
     },
     "user_tz": -540
    },
    "id": "X0WG7ZmK4ecm",
    "outputId": "b9a6a303-c948-4eda-8c7a-f2c9d3897b91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "text/smax\n",
      "- - -\n",
      "1\n",
      "text/kaden-channel\n",
      "- - -\n",
      "2\n",
      "text/livedoor-homme\n",
      "- - -\n",
      "3\n",
      "text/movie-enter\n",
      "- - -\n",
      "4\n",
      "text/sports-watch\n",
      "- - -\n",
      "5\n",
      "text/it-life-hack\n",
      "- - -\n",
      "6\n",
      "text/topic-news\n",
      "- - -\n",
      "7\n",
      "text/peachy\n",
      "- - -\n"
     ]
    }
   ],
   "source": [
    "for (i, directory) in enumerate(directories):\n",
    "    print(i)\n",
    "    print(directory)\n",
    "    print('- - -')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UpaX1NGW4eco"
   },
   "outputs": [],
   "source": [
    "texts, labels = [], []\n",
    "for (i, directory) in enumerate(directories):\n",
    "    #各ディレクトリ内のtxtファイルのパスをすべて取得\n",
    "    filepaths = glob('{}/*.txt'.format(directory))\n",
    "    # テキストを読み込んで、内容をtextに格納、ラベルも併せて格納\n",
    "    for filepath in filepaths:\n",
    "        with open(filepath, encoding='utf-8') as f:\n",
    "            text = ''.join(f.readlines()[2:])  # URL等の先頭２行を除いた各行の文章を連結（join）して格納\n",
    "            texts.append(text)\n",
    "            labels.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NhcJs6yz4ecq"
   },
   "source": [
    "たとえば、最後に格納された文書を確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12434,
     "status": "ok",
     "timestamp": 1569055687180,
     "user": {
      "displayName": "佐川史弥",
      "photoUrl": "",
      "userId": "17735252083618003950"
     },
     "user_tz": -540
    },
    "id": "XnBQfIjd4ecq",
    "outputId": "5146c0cf-67d3-4e96-c134-198e0e714ab1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'東京スカイツリーを好きな色でライトアップしちゃおう！「夜のスカイツリー 〜 ライトアップ時計デザイナー 無料」【iPhoneアプリ】\\n何色のライトアップが綺麗かな！？ \\n\\n賑わいをみせている東京スカイツリーですが、夜の顔ともいえるライトアップも綺麗ですよね。水色をベースとした「粋」、紫色をベースとした「雅」も印象がガラッと変わっていい感じになりますよね！\\n\\nこの色を使ってライトアップをしたらもっと綺麗になると思うんだけど…なんて思った方にオススメのアプリが今回紹介するiPhone向けアプリ「夜のスカイツリー 〜 ライトアップ時計デザイナー 無料」です！\\n\\nこのアプリを使うと好きな色で東京スカイツリーをライトアップさせることができます♪\\n\\n日付や時計も表示できる時計アプリとしても使うことができ、綺麗にカッコよくライトアップできたら壁紙に設定という使い方もできちゃいます。\\n\\nでは、早速、紹介してみたいと思います。\\n\\n\\nアプリを起動してしばし待ちます…。\\n\\n\\n待っていると徐々に明かりが灯ってきます。\\n\\n\\nそしてスカイツリーが現れます！\\nちなみにこのイルミネーションは「雅」紫色のデコレーションが綺麗ですね。\\n\\n\\nそしてこちらが「粋」ブルーのイルミネーションになっています。\\n\\n好きな色のイルミネーションを設定してみましょう！\\n画面をタップするとイルミネーションの色を細かく好きな色に設定することができます。\\n日付や時計も個々に色を変えることができ、右上の雅、粋をタップするとそれぞれの初期設定イルミネーションに上書き保存されます。\\n個々に色を変えるときは、左右の縦に並んだアイコンをタップして左下にあるカラーダイヤルで色を決め、右のダイヤルでグラデーションの濃さを決めていき右下の▶をタップしたら設定完了です。\\n\\n\\n\\n\\n\\n\\n\\n設定した色でスカイツリーがライトアップされました！\\n\\n\\n\\n\\n日付や時計、スカイツリーをタップして好きな場所に移動させることができます。\\n\\n色編集画面右上にある歯車にようなアイコンをタップすると時計や日付の大きさ、秒表示、日付の書式などの細かい設定をすることができるのでいろいろと設定を変えてみるのも楽しいですよ♪\\n\\n\\n\\n\\n\\n色編集画面の背景をスワイプすると用意されている10種類の背景から好きな物に変更することができます。\\n\\n\\n\\n\\n\\n時計の色をカラーダイヤルの一番右にして、隣のグラデーションダイヤルを1番左にするとスカイツリーだけが表示されているように見えるのでスクリーンショットを撮って待受にすることもできちゃいますよ！\\n\\n記事執筆：にゃんこ\\n\\nアプリ名：夜のスカイツリー 〜 ライトアップ時計デザイナー 無料\\n価格：無料\\nカテゴリ：エンターティメント\\n開発者：FANG Inc.\\nバージョン：1.0.0\\n条件：iPhone 3GS、iPhone 4、iPhone 4S、iPod touch（第3世代）、iPod touch (第4世代)、およびiPad に対応。iOS 5.0 以降が必要\\niTunes Store：http://itunes.apple.com/jp/app/id526433751?mt=8\\n\\n\\n\\n\\n\\n■関連リンク\\n・エスマックス（S-MAX）\\n・エスマックス（S-MAX） smaxjp on Twitter\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12429,
     "status": "ok",
     "timestamp": 1569055687180,
     "user": {
      "displayName": "佐川史弥",
      "photoUrl": "",
      "userId": "17735252083618003950"
     },
     "user_tz": -540
    },
    "id": "NmQXGsLy4ecs",
    "outputId": "c708d7ac-1298-4810-a508-b710645f370c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8LKhEds24ecu"
   },
   "source": [
    "## 文章から名詞のみを抽出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oOD5iP-M4ecu"
   },
   "source": [
    "前に作成した名詞抽出用の関数を使用して、文書全体で使用されている名詞を全てword_collectというリストに格納していきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  aptitude-common libboost-filesystem1.65.1 libboost-iostreams1.65.1\n",
      "  libboost-system1.65.1 libcgi-fast-perl libcgi-pm-perl libclass-accessor-perl\n",
      "  libcwidget3v5 libencode-locale-perl libfcgi-perl libhtml-parser-perl\n",
      "  libhtml-tagset-perl libhttp-date-perl libhttp-message-perl libio-html-perl\n",
      "  libio-string-perl liblocale-gettext-perl liblwp-mediatypes-perl\n",
      "  libparse-debianchangelog-perl libsigc++-2.0-0v5 libsub-name-perl\n",
      "  libtimedate-perl liburi-perl libxapian30\n",
      "Suggested packages:\n",
      "  aptitude-doc-en | aptitude-doc apt-xapian-index debtags tasksel\n",
      "  libcwidget-dev libdata-dump-perl libhtml-template-perl libxml-simple-perl\n",
      "  libwww-perl xapian-tools\n",
      "The following NEW packages will be installed:\n",
      "  aptitude aptitude-common libboost-filesystem1.65.1 libboost-iostreams1.65.1\n",
      "  libboost-system1.65.1 libcgi-fast-perl libcgi-pm-perl libclass-accessor-perl\n",
      "  libcwidget3v5 libencode-locale-perl libfcgi-perl libhtml-parser-perl\n",
      "  libhtml-tagset-perl libhttp-date-perl libhttp-message-perl libio-html-perl\n",
      "  libio-string-perl liblocale-gettext-perl liblwp-mediatypes-perl\n",
      "  libparse-debianchangelog-perl libsigc++-2.0-0v5 libsub-name-perl\n",
      "  libtimedate-perl liburi-perl libxapian30\n",
      "0 upgraded, 25 newly installed, 0 to remove and 4 not upgraded.\n",
      "Need to get 3974 kB of archives.\n",
      "After this operation, 16.0 MB of additional disk space will be used.\n",
      "Do you want to continue? [Y/n] ^C\n",
      "/bin/sh: 1: aptitude: not found\n",
      "Collecting mecab-python3==0.7\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/e9/bbf5fc790a2bedd96fbaf47a84afa060bfb0b3e0217e5f64b32bd4bbad69/mecab-python3-0.7.tar.gz (41kB)\n",
      "\u001b[K     |################################| 51kB 1.1MB/s eta 0:00:011\n",
      "\u001b[?25hBuilding wheels for collected packages: mecab-python3\n",
      "  Building wheel for mecab-python3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mecab-python3: filename=mecab_python3-0.7-cp36-cp36m-linux_x86_64.whl size=156371 sha256=eeb069de27247bb0113212c2a263f4c567d7347970570bf7cd9015a1d0f0c70f\n",
      "  Stored in directory: /root/.cache/pip/wheels/4c/07/3a/5f22ccc9f381f3bc01fa023202061cd1e0e9af855292f005dd\n",
      "Successfully built mecab-python3\n",
      "Installing collected packages: mecab-python3\n",
      "  Found existing installation: mecab-python3 0.996.2\n",
      "    Uninstalling mecab-python3-0.996.2:\n",
      "      Successfully uninstalled mecab-python3-0.996.2\n",
      "Successfully installed mecab-python3-0.7\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!apt install aptitude\n",
    "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
    "!pip install mecab-python3==0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FgFOBitZ4ecv"
   },
   "outputs": [],
   "source": [
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WF9Vuy6X4ec0"
   },
   "outputs": [],
   "source": [
    "mecab = MeCab.Tagger('-Ochasen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gw7sV9SO4ec2"
   },
   "outputs": [],
   "source": [
    "def get_nouns(text):\n",
    "    nouns = []\n",
    "    res = mecab.parse(text)\n",
    "    words = res.split('\\n')[:-2] #EOSと空白部分の削除\n",
    "    for word in words:\n",
    "        part = word.split('\\t')\n",
    "        if '名詞' in part[3]:\n",
    "            nouns.append(part[0])\n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PQQHwS_r4ec3"
   },
   "outputs": [],
   "source": [
    "word_collect = []\n",
    "for text in texts:\n",
    "    nouns = get_nouns(text)\n",
    "    word_collect.append(nouns)\n",
    "    \n",
    "# ワンライナーで下記のように書いてもOK\n",
    "# word_collect = [ get_nouns(text) for text in texts ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZQVyAfW-4ec5"
   },
   "source": [
    "## BoWに変換\n",
    "\n",
    "前回と同様に`gensim`を使いましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s5xpmljj4ec6"
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, matutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SnKRPOhX4ec9"
   },
   "source": [
    "まずはBoW用の辞書を作りましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hli9iJGf4ec-"
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(word_collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 83714,
     "status": "ok",
     "timestamp": 1569055758504,
     "user": {
      "displayName": "佐川史弥",
      "photoUrl": "",
      "userId": "17735252083618003950"
     },
     "user_tz": -540
    },
    "id": "uWW51njw4edB",
    "outputId": "68ced26c-8d8b-4a96-a899-541db71f1b6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81051"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-bmyQyvc4edC"
   },
   "source": [
    "今回は`58205`単語あるようです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mq7Kxq7D4edC"
   },
   "source": [
    "このままでも良いのですが、出現回数が多すぎたり引きすぎる単語をフィルタリングすると、特徴のある単語のみに絞ることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ggIH7SZ94edD"
   },
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 83912,
     "status": "ok",
     "timestamp": 1569055758712,
     "user": {
      "displayName": "佐川史弥",
      "photoUrl": "",
      "userId": "17735252083618003950"
     },
     "user_tz": -540
    },
    "id": "QkDaKDo34edE",
    "outputId": "5efbe914-f68f-46dd-a89f-b5bba641166f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6697"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dMZAO8vI4edG"
   },
   "source": [
    "このように全体で20回以上出現しない単語はフィルタリングすることで、`6586`単語に抑えることができました。\n",
    "辞書作成に多少時間がかかるため、あとから使えるようにこの段階で保存しておくと良いでしょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 83908,
     "status": "ok",
     "timestamp": 1569055758713,
     "user": {
      "displayName": "佐川史弥",
      "photoUrl": "",
      "userId": "17735252083618003950"
     },
     "user_tz": -540
    },
    "id": "QSDFN-ck4edG",
    "outputId": "18ea3a1d-59a2-4514-bf18-03521e9b83d7"
   },
   "outputs": [],
   "source": [
    "# 後から使えるように保存しておく\n",
    "dictionary.save_as_text('livedoordic.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1ESBUKL4edI"
   },
   "source": [
    "それではこの作成した辞書を使って、BoWに変換しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7LGOzBXH4edI"
   },
   "outputs": [],
   "source": [
    "n_words = len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 86182,
     "status": "ok",
     "timestamp": 1569055760996,
     "user": {
      "displayName": "佐川史弥",
      "photoUrl": "",
      "userId": "17735252083618003950"
     },
     "user_tz": -540
    },
    "id": "PzLPNSwG4edJ",
    "outputId": "f16a6d87-f453-4ee6-82e0-bc0ef7df415e"
   },
   "outputs": [],
   "source": [
    "# BOWによる特徴ベクトルの作成\n",
    "x = []\n",
    "for nouns in word_collect:\n",
    "    bow_id = dictionary.doc2bow(nouns)\n",
    "    bow = matutils.corpus2dense([bow_id], n_words).T[0]\n",
    "    x.append(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-kqX1tIv4edL"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t1JCtSPU4edN"
   },
   "outputs": [],
   "source": [
    "x = np.array(x)\n",
    "t = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 86167,
     "status": "ok",
     "timestamp": 1569055760998,
     "user": {
      "displayName": "佐川史弥",
      "photoUrl": "",
      "userId": "17735252083618003950"
     },
     "user_tz": -540
    },
    "id": "jloya6nF4edP",
    "outputId": "0df4a7de-9a0d-4af7-d362-a3d6f8f6895c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6505, 6697)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 86162,
     "status": "ok",
     "timestamp": 1569055760998,
     "user": {
      "displayName": "佐川史弥",
      "photoUrl": "",
      "userId": "17735252083618003950"
     },
     "user_tz": -540
    },
    "id": "qMsy5b0Z4edR",
    "outputId": "02843358-fe8d-494e-de4b-4ea5fc23a501"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6505,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fuKk3RAv4edS"
   },
   "source": [
    "こちらのように機械学習で使用できる形式へ変換できました。\n",
    "あとは、単純な分類問題であるため、NNでクラス分類することで実装できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U7vQjlem4edT"
   },
   "source": [
    "## 演習課題\n",
    "\n",
    "得られたデータセットを使用してクラス分類を行うNNのモデルを作成せよ。\n",
    "\n",
    "条件\n",
    "\n",
    "- seedは0で固定\n",
    "- 全体の70%が訓練データ、残りの30%が検証データ（ランダムに分割）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnun9Wge8Pz2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def reset_seed(seed=0):\n",
    "    \n",
    "    os.environ['PYTHONHASHSEED'] = '0'\n",
    "    random.seed(seed) #　random関数のシードを固定\n",
    "    np.random.seed(seed) #numpyのシードを固定\n",
    "    tf.random.set_seed(seed) #tensorflowのシードを固定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RCx7YtWm4edT"
   },
   "outputs": [],
   "source": [
    "# Pythonの挙動を整えるライブラリのインポート\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# TensorFlow と tf. のインポート\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import models, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 89706,
     "status": "ok",
     "timestamp": 1569055764562,
     "user": {
      "displayName": "佐川史弥",
      "photoUrl": "",
      "userId": "17735252083618003950"
     },
     "user_tz": -540
    },
    "id": "AFTauuGs_hoV",
    "outputId": "4e62426b-2423-41e8-d6a6-9411d573fe44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-zgIbPc14edW"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 訓練データと検証データの分割\n",
    "train_x, val_x, train_t, val_t = train_test_split(x, t, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8LAB1Hws4edY"
   },
   "outputs": [],
   "source": [
    "# シードの固定\n",
    "reset_seed(0)\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(200, input_shape=(6586, ), activation='relu'))\n",
    "model.add(layers.Dense(9, activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=0.01)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 117527,
     "status": "ok",
     "timestamp": 1569055792403,
     "user": {
      "displayName": "佐川史弥",
      "photoUrl": "",
      "userId": "17735252083618003950"
     },
     "user_tz": -540
    },
    "id": "Zfcnlpx_4edZ",
    "outputId": "b3dde849-799f-4b78-ab51-f38cded1d35b"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_input to have shape (6586,) but got array with shape (6697,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-fe58b41fb165>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           validation_data=(val_x, val_t))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         steps=steps)\n\u001b[0m\u001b[1;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[1;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2470\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2471\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2472\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    572\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    575\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_input to have shape (6586,) but got array with shape (6697,)"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_x, train_t,\n",
    "          batch_size=100,\n",
    "          epochs=50,\n",
    "          verbose=1,\n",
    "          validation_data=(val_x, val_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X20eO0fA4edb"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 117958,
     "status": "ok",
     "timestamp": 1569055792843,
     "user": {
      "displayName": "佐川史弥",
      "photoUrl": "",
      "userId": "17735252083618003950"
     },
     "user_tz": -540
    },
    "id": "6jJXicXJ4edc",
    "outputId": "af6181c2-f024-4c83-aa5c-c934ceb67856"
   },
   "outputs": [],
   "source": [
    "# 学習結果をPandasのDataFrame型で読み込みます。\n",
    "results = pd.DataFrame(history.history)\n",
    "\n",
    "# accuracy（精度）を表示\n",
    "results[['accuracy', 'val_accuracy']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 118384,
     "status": "ok",
     "timestamp": 1569055793274,
     "user": {
      "displayName": "佐川史弥",
      "photoUrl": "",
      "userId": "17735252083618003950"
     },
     "user_tz": -540
    },
    "id": "o5sPfY9X4ede",
    "outputId": "68983346-b0c1-4061-9255-ae450640cc09"
   },
   "outputs": [],
   "source": [
    "# loss（損失関数）を表示\n",
    "results[['loss', 'val_loss']].plot()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "TF_document_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
